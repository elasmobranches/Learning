{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3단원\n",
    "\n",
    "### knr\n",
    "\n",
    "회귀는 정해진 클래스가 아닌 임의의 수치를 출력한다\n",
    "\n",
    "사이킷런에서 사용할 훈련 세트는 2차원 배열이어야 한다\n",
    "\n",
    "이를 위해 reshape를 사용해야 할 수도 있다\n",
    "\n",
    "분류에서 score 값은 테스트에 있는 샘플을 정확하게 분류한 개수의 비율\n",
    "\n",
    "회귀에서 score 값은 R^2 결정계수\n",
    "\n",
    "훈련 세트에서 점수가 굉장히 좋았는데 테스트 세트에서는 점수가 굉장히 나쁘다면 모델이 과대적합 된 것이고 훈련 세트보다 테스트 세트의 점수가 높거나 두 점수가 모두 너무 낮다면 이는 과소적합 되었다고 말 한다\n",
    "\n",
    "KNeighborsRegressor에서 더 복잡하게 만들려면 n_neighbors를 낮게 만들면 된다\n",
    "\n",
    "mean_absolute_error는 회귀 모델의 평균 절댓값 오차를 계산한다\n",
    "\n",
    "- 과대적합이 왜 되는거지?\n",
    "    \n",
    "    KNeighborsRegressor는 KNN(k-nearest neighbor) 알고리즘 계열에 속하는 회귀 작업에 사용되는 알고리즘 유형입니다. 주어진 데이터 포인트의 k-최근접 이웃을 찾고 해당 값을 사용하여 해당 포인트의 값을 예측하는 방식으로 작동합니다. KNeighborsRegressor에서 고려할 이웃 수를 나타내는 매개변수 n_neighbors를 낮게 설정하면 다음과 같은 이유로 과적합이 발생할 수 있습니다.\n",
    "    \n",
    "    1. **높은 분산**: 이웃 수가 적으면 모델이 훈련 데이터의 노이즈에 더 민감해집니다. 훈련 데이터를 너무 가깝게 맞춰서 기본 패턴을 학습하기보다는 데이터의 특이성과 변동을 포착하려고 합니다. 이는 높은 분산 모델로 이어질 수 있습니다. 즉, 훈련 데이터에서는 성능이 좋지만 보이지 않는 데이터에서는 성능이 좋지 않습니다.\n",
    "    2. **복잡성 증가**: 'n_neighbors'가 낮으면 모델의 결정 경계가 특히 고차원 공간에서 더욱 복잡하고 불규칙해집니다. 이러한 복잡성으로 인해 모델이 기능과 대상 변수 간의 기본 관계가 아닌 노이즈를 캡처할 수 있습니다.\n",
    "    3. **일반화 불량**: 모델이 훈련 데이터에 너무 가깝게 적합하기 때문에 보이지 않는 데이터에 대해서는 잘 일반화되지 않을 수 있습니다. 데이터의 기본 구조를 캡처하지 못하고 대신 훈련 예제를 기억하므로 보이지 않는 새로운 데이터에 대한 성능이 저하됩니다.\n",
    "    \n",
    "    KNeighborsRegressor의 과적합을 완화하려면 'n_neighbors' 값을 늘려보세요. 고려되는 이웃의 수를 늘리면 모델이 노이즈에 더욱 강력해지고 보이지 않는 데이터에 대해 더 잘 일반화되는 경향이 있습니다. 그러나 'n_neighbors'를 너무 많이 늘리면 모델이 지나치게 단순화되어 데이터의 기본 패턴을 포착하지 못하는 과소적합이 발생할 수 있습니다. 따라서 실험이나 교차 검증을 통해 균형을 맞추고 **`n_neighbors`**에 대한 적절한 값을 선택하는 것이 중요합니다.\n",
    "    \n",
    "\n",
    "### 선형 회귀\n",
    "\n",
    "KNR은 데이터 셋 내의 정보로만 값을 추정하기에 기존의 값보다 훨씬 높거나 낮은 feature가 주어진다면 제대로 된 결과물이 나오지 않을 가능성이 있다 그래서 선형 회귀가 필요하다\n",
    "\n",
    "독립변수와 종속변수 간의 선형 관계를 가정하는 알고리즘\n",
    "\n",
    "coef_ 와 intercept_는 각각 계수, y절편이다. 이를 머신러닝 알고리즘이 찾은 값이라는 의미로 모**델 파라미터** 라고 부른다\n",
    "\n",
    "### 다항 회귀\n",
    "\n",
    "이건 2차 방정식인데 비선형 아닌가???\n",
    "\n",
    "제곱값을 다른 변수로 치환 가능하다! 그렇다면 선형 관계로 표현 가능하다\n",
    "\n",
    "**다항식을 이용한 선형 회귀를 다항 회귀라고 한다** \n",
    "\n",
    "### 다중 회귀\n",
    "\n",
    "이전에는 하나의 특성을 이용하여 선형 회귀 모델을 학습시켰다. \n",
    "\n",
    "여러 개의 특성을 사용한 선형 회귀를 **다중 회귀**라고한다\n",
    "\n",
    "사이킷런에는 transformer라고 부르는 특성을 만들거나 전처리하는 클래스가 있다\n",
    "\n",
    "PolynimialFeatures를 통해 특성을 늘릴 수 있다\n",
    "\n",
    "poly 또한 fit 이후에 transform이 되어야 한다\n",
    "\n",
    "**참고로 transform 하는데 target data가 필요하지 않다**\n",
    "\n",
    "### 규제(regularization)\n",
    "\n",
    "머신러닝 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 훼방하는 것\n",
    "\n",
    "규제를 적용하기 전에  계수 값의 크기가 서로 많이 다르면 공정하게 제어되지 않을 것이다. 따라서 규제를 적용하기 전에 **정규화를 먼저 해야한다**\n",
    "\n",
    "### 선형 회귀 모델에 규제를 추가한 모델 - 릿지 라쏘\n",
    "\n",
    "두 알고리즘 모두 계수의 크기를 줄이며 라쏘는 아예 0으로 만드는 것이 가능하다\n",
    "\n",
    "모델 객체를 만들 때 alpha 매개변수로 규제의 강도를 조절한다. alpha 값이 크면 규제 강도가 세지므로 계수 값을 더 줄이고 조금 더 과소적합되도록 유도한다 alpha 값이 작으면 계수를 줄이는 역할이 줄어들고 과대적합 될 가능성이 커진다\n",
    "\n",
    "또한 이 alpha 값은 사전에 우리가 지정해 줘야 하는 하이퍼 파라미터라고 한다.\n",
    "\n",
    "다중 분류는 클래스마다 z값을 하나씩 계산한다. 가장 높은 z값을 출력하는 클래스가 예측 클래스가 된다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
