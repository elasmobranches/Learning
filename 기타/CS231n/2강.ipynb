{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2\n",
    "\n",
    "대부분의 수치계산 머신러닝은 넘파이로 vectorized tensor 계산 한다\n",
    "\n",
    "이 수업은 이미지 분류에 관심을 가진다\n",
    "\n",
    "**Image Classification - 이미지 분류는 컴퓨터 비전의 주요 문제, 이미지에 해당하는** \n",
    "\n",
    "**label값을 예측하는 것 ⇒**\n",
    "\n",
    "**1. 입력 이미지를 받는다. 2. 이미지를 보고 어떤 카테고리에 속할지 고른다.: 카테고리(개, 고양이, 비행기, 말 등)는 시스템 상에서 미리 정해놓은 것**\n",
    "\n",
    "**의미론적 차이(Semantic Gap)**\n",
    "\n",
    "우리는 사진을 직관적으로 판별할 수 있지만 컴퓨터에게 이미지는 단지 거대한 숫자 집합에 불과하다 \n",
    "\n",
    "고양이 사진이라는 사실과 실제 '컴퓨터'가 보는 픽셀값과는 큰 차이가 존재하는데,\n",
    "\n",
    "이것이 바로 Semantic Gap\n",
    "\n",
    "컴퓨터가 이미지를 인식하는데에 여러 Challenge\n",
    "\n",
    "**Viewpoint variation, Illumination, Deformation, Occlusion, Background Clutter, Intraclass variation**\n",
    "\n",
    "이러한 상황에서도 고양이를 식별할 수 있는 방법이 제한적으로 존재한다 !!\n",
    "\n",
    "직관적이고 명시적인 알고리즘은 존재하지 않는다 + edges도 중요\n",
    "\n",
    "---\n",
    "\n",
    "**Data-driven approach(**데이터 중심 접근방법)\n",
    "\n",
    "손으로 규칙을 써내려가는 것 대신에 인터넷에서 엄청 많은 데이터를 수집한다\n",
    "\n",
    "이를 이용해 classifier 사용 가능\n",
    "\n",
    "**1. 이미지와 레이블로 구성된 데이터를 수집**\n",
    "\n",
    "**2. 수집된 데이터로 이미지 분류기를 학습시킴**\n",
    "\n",
    "**3. 테스트 이미지에 대해서 학습시킨 이미지 분류기를 평가**\n",
    "\n",
    "- 함수 1 = **Train 함수 (**입력 : 이미지와 레이블 출력 : 모델)\n",
    "- 함수 2 = **Predict 함수 (**입력 : 모델 출력 : 이미지의 예측값)\n",
    "\n",
    "### NN(Nearest Neighbor)\n",
    "\n",
    "Train 에서 모든 학습 데이터를 기억한다\n",
    "\n",
    "Predict에서는 새로운 이미지가 들어오면 기존 학습 데이터를 비교하여 가장 유사한 이미지로 labeling\n",
    "\n",
    "\n",
    "그러나 자세히 보면 정확도가 높지 않다는 것을 알 수 있다\n",
    "\n",
    "## 이미지 쌍이 있을 때 비교하는 방법\n",
    "\n",
    "### L1 distance(맨하탄 거리)\n",
    "\n",
    "두 점 사이의 거리의 차이에 절댓값을 취한다\n",
    "\n",
    "\n",
    "\n",
    "### L2 distance(유클리디안 거리)\n",
    "\n",
    "제곱근 개념\n",
    "\n",
    "만약 특징 벡터의 각각 요소들이 개별적인 의미를 가지고 있다면(ex. 키, 몸무게)\n",
    "\n",
    "L1 distance가 더 잘 어울릴 수 있다.\n",
    "\n",
    "하지만 특징 벡터가 일반적인 벡터이고, 요소들 간의 실질적인 의미를 잘 모르는 경우라면 아마도 L2 distance가 더 잘 어울릴 수 있다.\n",
    "\n",
    "**NN의 문제점**\n",
    "\n",
    "\n",
    "\n",
    "1. 가장 가운데에 주변은 전부 초록색인데 중간에 노란색이 끼어있음\n",
    "2. 초록색이 파란색 영역 침범\n",
    "\n",
    "이를 보완하는 **KNN**\n",
    "\n",
    "distance metric을 이용해서 가까운 이웃을 K개만큼 찾고 이웃끼리 투표하는 방법\n",
    "\n",
    "가장 가까운 K개의 이웃이 초록색이라면, 테스트 이미지는 초록색 영역으로 결정되는 원리\n",
    "\n",
    "K는 하이퍼파라미터로서 다양한 값을 대입해보며 최적의 값을 찾아야 한다\n",
    "\n",
    "**가장 최적의 하이퍼 파라미터를 찾기 위해 사용하는 방식 - Cross-Validation**\n",
    "\n",
    "\n",
    "이거는 데이터가 적을 때 사용\n",
    "\n",
    "딥러닝은 학습 계산량이 애초에 많아서 이렇게까지는 잘 안함\n",
    "\n",
    "이미지 분류에 KNN 잘 안 쓴다\n",
    "\n",
    "- 계산하는 데에 시간이 오래걸림\n",
    "- 현실적으로 '거리'를 사용해서 정확한 예측을 하기 어려움\n",
    "    - L1은 좌표계에 의존적\n",
    "        \n",
    "        L2가 이미지 유사도 측정에는 적합하지 않다\n",
    "        \n",
    "- 차원의 저주: 차원이 증가함에 따라 필요한 학습데이터의 양이 기하급수적으로 증가고차원의 이미지라면 모든 공간을 조밀하게 메울 만큼의 데이터를 모으는 일은 현실적으로 불가능\n",
    "\n",
    "\n",
    "\n",
    "최적의 하이퍼파라미터를 찾으려고 계속 돌리는게 일반적이지는 않다?!\n",
    "\n",
    "선형 분류(**Linear Classification)** - NN과 CNN의 기반 알고리즘\n",
    "\n",
    "parametric model의 기초\n",
    "\n",
    "\n",
    "\n",
    "parametric 접근법에서는 train 데이터의 정보를 요약해서 파라미터 w에 모아주는 것. \n",
    "\n",
    "따라서 test time에 더이상 트레이닝 데이터를 직접 비교하지 않고, W만 사용할 수 있게 됨\n",
    "\n",
    "딥러닝이란 여기서 이 함수 f를 잘 설계하는 일\n",
    "\n",
    "선형 분류란? f=wx\n",
    "\n",
    "요약된 정보를 파라미터에 모아준다\n",
    "\n",
    "템플릿 매칭\n",
    "\n",
    "**선형 분류는 각 클래스에 대해서 하나의 템플릿만 학습한다는 것이 문제이다**\n",
    "\n",
    "이미지를 고차원 공간의 한 점으로 생각해보기\n",
    "\n",
    "Linear classifier은 아래와 같이 각 클래스를 구분시켜주는 선형 boundary 역할\n",
    "\n",
    "\n",
    "\n",
    "아래와 같은 데이터 셋은 선형 분류하기 힘들다. parity problem류, multimodal problem은 어렵다\n",
    "\n",
    "\n",
    "\n",
    "- 멀티모달\n",
    "    - 텍스트, 이미지, 음성, 영상 등 여러 종류의 데이터를 함께 사용하는 분야. 각 종류의 데이터의 특징에 따라 데이터 공간의 차원이 높아지게 되고, 이는 curse of dimensionality (차원의 저주)가 일어나 성능저하로 이어지기 쉽다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
