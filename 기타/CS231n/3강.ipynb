{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture3\n",
    "\n",
    "어떤 W가 좋은지 나쁜지 정량화 할 방법이 필요 이것이 바로 손실함수\n",
    "\n",
    "이번 수업에서는 Image classification에 필요한 손실함수\n",
    "\n",
    "- Multiclass SVM Loss\n",
    "- Softmax Classifier\n",
    "\n",
    "가장 덜 구린 W가 무엇인지 찾고 싶은 것 이것이 **최적화 과정**\n",
    "\n",
    "x와 행렬 W를 입력 받아서 y를 예측하는 것이다\n",
    "\n",
    "multi-class SVM Loss\n",
    "\n",
    "**학생질문: S_Yi ? S가 뭔데?**\n",
    "\n",
    "S는 분류기의 출력으로 나온 예측된 스코어\n",
    "\n",
    "예를 들어 1이 고양이고 2가 개면 S_1은 고양이 스코어 S_2는 개 스코어\n",
    "\n",
    "Y_i는 이미지의 실제 정답 카테고리. 정수 값이다\n",
    "\n",
    "⇒ S_Y_i는 트레이닝 셋의 i번째 이미지의 정답 클래스의 스코어\n",
    "\n",
    "Loss가 말하고자 하는 것은 정답 스코어가 다른 스코어들보다 높으면 좋다는 것\n",
    "\n",
    "정답 스코어는 safety margin을 두고 다른 스코어들보다 훨씬 더 높아야 한다\n",
    "\n",
    "zero on notation 사용\n",
    "\n",
    "SVM Loss\n",
    "\n",
    "\n",
    "⇒ 우리의 분류기가 5.3점 만큼 이 트레이닝 셋을 구리게 분류하고 있다는 **정량적 지표**\n",
    "\n",
    "근데 safety margin이 왜 1이냐?\n",
    "\n",
    "우리는 사실 손실함수의 스코어가 정확히 몇인지 신경쓰지 않는다\n",
    "\n",
    "우리가 궁금한 것은 **여러 스코어 간의 상대적인 차이**\n",
    "\n",
    "**정답 스코어가 다른 스코어에 비해 얼마나 더 큰 스코어를 가지고 있는가**\n",
    "\n",
    "손실함수의 **최솟값은 0 최대값은 무한대** 일 것\n",
    "\n",
    "처음 트레이닝을 시작할 때 Loss가 C-1이 아니라면(디버깅) 고쳐야 한다 →실제로 유용\n",
    "\n",
    "\n",
    "\n",
    "**squared hinge loss → 조금 잘못된 것과 많이 잘못된 것을 구별하기 위해 사용한다**\n",
    "\n",
    "**3) 만약 W대신 kW라 한다면**\n",
    "\n",
    "loss값들 또한 k배가 될 것이다. \n",
    "\n",
    "4) L=0이 되는 W는 unique한가?\n",
    "\n",
    "그렇지 않다. 무한대만큼 존재한다. 그렇다면 W와 2W중 뭐가 좋은지 알 수 없다\n",
    "\n",
    "### 그래서 이를 위해 Regularization이라는 개념이 도입된다\n",
    "\n",
    "\n",
    "\n",
    "정규화 페널티 R\n",
    "\n",
    "1) 복잡해지는것을 막는다 2) 가장 보편적인 것은 L2 유클리디안 norm W의 유클리디안 norm에 페널티 \n",
    "\n",
    "\n",
    "\n",
    "우변의 왼쪽부분은 Data loss로 training data에 대한 loss를 의미한다. 그리고 오른쪽 부분은 Regulaization부분으로 모델이 training data에 너무 fit하지 않도록, W값을 조정하는 역할을 한다. \n",
    "\n",
    "즉, 모델이 너무 복잡하지 않도록 규제하는 역할을 한다. *λ*는 hyperparameter이다.\n",
    "\n",
    "\n",
    "L1 L2는 복잡도를 정의하는 방법이 다르다\n",
    "\n",
    "```\n",
    "\tx = [1, 1, 1, 1] w1 = [1, 0, 0, 0] w2 = [0.25, 0.25, 0.25, 0.25]\n",
    "```\n",
    "\n",
    "전자와 후자는 Wx가 같은 값을 갖지만\n",
    "\n",
    "전자의 L2는 1+0+0+0\n",
    "\n",
    "후자의 L2는 1/16+ 1/16+ 1/16+ 1/16+=1/4\n",
    "\n",
    "**후자가 더 작은 Loss**\n",
    "\n",
    "사실 L1, L2를 쓸 때 그 손실 함수의 차이가 유의미하게 나지는 않지만 \n",
    "\n",
    "그 차이의 원인을 아는 건 중요\n",
    "\n",
    "딥러닝 - softmax Loss 많이 사용\n",
    "\n",
    "### Softmax classifier\n",
    "\n",
    "SVM의 경우에는 조건만 만족하면 그 이상으로 좋아질 수 없다(가장 좋은 경우가 0이므로)\n",
    "\n",
    "하지만 Softmax는 확률을 이용하기 때문에 지속적으로 좋아질 수 있는 가능성이 있다\n",
    "\n",
    "확률이 1이 될 때까지 노력한다\n",
    "\n",
    "multinomial logistic Regression\n",
    "\n",
    "정답 클래스에 해당하는 확률이 1이 되게 한다 log 최대화가 확률 최대화 보다 쉽다\n",
    "\n",
    "얼마나 구린지를 원하기에 log에 -1을 붙인다\n",
    "\n",
    "\n",
    "\n",
    "이것의 최대와 최소는?\n",
    "\n",
    "-log함수의 그래프이고 log의 인자가 0-1의 범위를 가지므로 최솟값은 0 최대값은 무한대\n",
    "\n",
    "완벽히 분류하면 Loss = 0 실제 스코어는 몇이어야?? 아마 극단적으로 높아야 할것이다, 나머지는 -무한대\n",
    "\n",
    "얼마나 구린지를 파악하기 위한 방법이 서로 다르다\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "⇒ 정말 구린 방법\n",
    "\n",
    "### Optimization\n",
    "\n",
    "**W값을 찾기 위해 랜덤하게 값을 부여하는 Random search는 매우 좋지 않은 방법이다. 따라서 좋은 W값을 찾기 위해 gradient를 사용한다.**\n",
    "\n",
    "\n",
    "\n",
    "gradeint 계산을 구현할 때는 분석적인 방법을 이용한다\n",
    "\n",
    "수치적 gradient는 디버깅에 쓰인다\n",
    "\n",
    "\n",
    "\n",
    "- **Numerical gradient**: 근사, 느림, 쉽게 작성가능(일일이 f(x+h)-f(x) / h)\n",
    "- **Analytic gradient**: 정확함, 빠름, error-prone(오류가 발생하기 쉬움)→(미분 공식 사용)\n",
    "\n",
    "\n",
    "\n",
    "update rule\n",
    "\n",
    "미니배치 경사법\n",
    "\n",
    "\n",
    "\n",
    "전체의 example을 보고 한 step을 가는 것은 비효율적이고 expensive할 수 있다. 따라서 minibatch를 사용하여 근사한다.\n",
    "\n",
    "⇒ 매번 loss를 계산할 때, 우리 training set에서 랜덤으로 조금만 가져와서, 그 사진들에 대한 loss를 계산하고, weight를 계산\n",
    "\n",
    "\n",
    "특징 변환\n",
    "\n",
    "컬러 히스토그램 HOG - edge정보를 파악"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
